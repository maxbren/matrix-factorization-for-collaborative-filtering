{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **{'family': 'DejaVu Sans', 'size'   : 16})\n",
    "matplotlib.rc('figure', figsize=(15, 7.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_toy_problem = False  # use toy problem or real dataset\n",
    "K = 32  # num latent features for U and V\n",
    "train_frac = 0.7  # fraction of R to use as train vs val\n",
    "num_epochs = 100\n",
    "lr = 0.25  # learning rate\n",
    "'''\n",
    "4 models: \n",
    "    1. From-scratch linear: 'scratch-linear'\n",
    "    2. Pytorch linear: 'pytorch-linear'\n",
    "    3. Pytorch non-linear (simple): 'pytorch-nonlinear-simple'\n",
    "    4. Pytorch non-linear (complex): 'pytorch-nonlinear-complex'\n",
    "''' \n",
    "# Running multiple models will plot the performance on the same plot\n",
    "model_definitions = ['pytorch-nonlinear-complex']  # can specify multiple models\n",
    "run_all_models = True  # or can just set this to True to run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Toy problem\n",
    "- Underlying model: Each item has a feature of is action, is comedy, is rated R, is big-budget\n",
    "\n",
    "Item 1: action, rated R\n",
    "Item 2: action, comedy, rated R, big-budget\n",
    "Item 3: comedy, big-budget\n",
    "Item 4: big-budget\n",
    "\"\"\"\n",
    "def make_toy_data():\n",
    "    R = np.array([\n",
    "        [4, 4, 3, 4],  # likes action, dislikes comedy, likes big-budget\n",
    "        [1, 2, 3, 2],  # likes comedy, dislikes action, dislikes R, dislikes big-budget\n",
    "        [4, 4, 3, 3],  # likes R\n",
    "        [4, 4, 3, 2],  # likes action, likes comedy, dislikes big-budget\n",
    "        [2, 4, 5, 4],  # dislikes action, likes comedy, likes big-budget\n",
    "    ], dtype=np.float32)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(use_toy_problem):\n",
    "    if use_toy_problem:\n",
    "        return make_toy_data()\n",
    "    # ow Movie lens dataset, made smaller\n",
    "    df = pd.read_csv('ratings.csv').astype(int)\n",
    "    num_users = len(df.userId.unique())\n",
    "    num_items = len(df.movieId.unique())\n",
    "    user_df_dict = dict(zip(df.userId.unique(), list(range(num_users))))\n",
    "    items_df_dict = dict(zip(df.movieId.unique(), list(range(num_items))))\n",
    "    R = np.full((num_users, num_items), np.nan, dtype=np.float32)\n",
    "    for indx, df_row in df.iterrows():\n",
    "        R[user_df_dict[df_row.userId], items_df_dict[df_row.movieId]] = float(df_row.rating)\n",
    "    # Make R smaller if you want\n",
    "#     R = R[:, :610]\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get/make dataset\n",
    "R = get_dataset(use_toy_problem)\n",
    "num_users, num_items = R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_and_val_splits(R, num_rows, num_cols, train_frac):\n",
    "    nan_full = lambda: np.full((num_rows, num_cols), np.nan)\n",
    "    R_train, R_val = nan_full(), nan_full()\n",
    "    did_train, did_val = False, False\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if np.random.random() < train_frac:\n",
    "                R_train[row, col] = R[row, col]\n",
    "                did_train = True\n",
    "            else:\n",
    "                R_val[row, col] = R[row, col]\n",
    "                did_val = True\n",
    "    assert did_train and did_val\n",
    "    return R_train, R_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/val splits\n",
    "R_train, R_val = make_train_and_val_splits(R, num_users, num_items, train_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_to_np(t):\n",
    "    if not torch.is_tensor(t):\n",
    "        return t\n",
    "    return t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationLinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.is_linear = True\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def calculate_loss(self, R, R_hat):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def optimize(self, loss, R, R_hat, N):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def mask_out_nans(R, R_hat):\n",
    "        lib = np if isinstance(R, np.ndarray) else torch\n",
    "        R_mask = lib.isnan(R)\n",
    "        R[R_mask] = R_hat[R_mask]\n",
    "        N = (R.shape[0] * R.shape[1]) - R_mask.sum()\n",
    "        return R, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationCustomLinearModel(MatrixFactorizationLinearModel):\n",
    "    def __init__(self, num_users, num_items, K, lr):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.U = MatrixFactorizationCustomLinearModel.xavier_uniform_array((num_users, K))\n",
    "        self.V = MatrixFactorizationCustomLinearModel.xavier_uniform_array((num_items, K))\n",
    "    \n",
    "    @staticmethod\n",
    "    def xavier_uniform_array(arr_shape):\n",
    "        limit = sqrt(6. / sqrt(arr_shape[0] + arr_shape[1]))\n",
    "        weights = np.random.uniform(-limit, limit, size=arr_shape)\n",
    "        return weights\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.U @ self.V.T\n",
    "        \n",
    "    @staticmethod\n",
    "    def _calc_mse(R, R_hat, N):\n",
    "        return np.square(R - R_hat).sum() / N\n",
    "    \n",
    "    def calculate_loss(self, R, R_hat):\n",
    "        # Mask out nans by setting to R_hat values\n",
    "        R, N = MatrixFactorizationLinearModel.mask_out_nans(R, R_hat)\n",
    "        # MSE and masked R\n",
    "        loss = MatrixFactorizationCustomLinearModel._calc_mse(R, R_hat, N)\n",
    "        return loss, loss, R, N\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calc_mse_gradient(R, R_hat, M, N):\n",
    "        return (R - R_hat)@M * -2. / N\n",
    "    \n",
    "    def optimize(self, loss, R, R_hat, N):\n",
    "        # Calc gradient of MSE for each factor matrix ie parameter\n",
    "        # MSE grad\n",
    "        U_grad = MatrixFactorizationCustomLinearModel._calc_mse_gradient(R, R_hat, self.V, N)\n",
    "        V_grad = MatrixFactorizationCustomLinearModel._calc_mse_gradient(R.T, R_hat.T, self.U, N)\n",
    "        # Update U and V based on gradient and step size\n",
    "        self.U -= self.lr * U_grad\n",
    "        self.V -= self.lr * V_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationPytorchLinearModel(MatrixFactorizationLinearModel):\n",
    "    def __init__(self, num_users, num_items, K, lr):\n",
    "        super().__init__()\n",
    "        self.U = torch.nn.Parameter(torch.zeros((num_users, K)))\n",
    "        self.V = torch.nn.Parameter(torch.zeros((num_items, K)))\n",
    "        torch.nn.init.xavier_uniform_(self.U)\n",
    "        torch.nn.init.xavier_uniform_(self.V)\n",
    "        \n",
    "        self.MSE = torch.nn.MSELoss(reduction='sum')\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.U @ self.V.T\n",
    "    \n",
    "    def calculate_loss(self, R, R_hat):\n",
    "        R = torch.tensor(R, dtype=torch.float)\n",
    "        # Mask out nans by setting to R_hat values\n",
    "        R, N = MatrixFactorizationLinearModel.mask_out_nans(R, R_hat)\n",
    "        t_loss = self.MSE(R, R_hat) / N\n",
    "        np_loss = convert_tensor_to_np(t_loss)\n",
    "        return t_loss, np_loss, R, N\n",
    "    \n",
    "    def optimize(self, loss, R, R_hat, N):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(MF_model, R_train, R_val, num_epochs):\n",
    "    train_losses, val_losses = [], []\n",
    "    # Training\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        # Train\n",
    "        # Reconstruct R_hat from latent factor matrices\n",
    "        R_hat = MF_model.forward()\n",
    "        # Calc MSE loss of this reconstruction\n",
    "        # And create a nan masked R_train for future use\n",
    "        train_loss, np_train_loss, R_train_masked, N = MF_model.calculate_loss(R_train.copy(), R_hat)\n",
    "        # Calc grad and update\n",
    "        MF_model.optimize(train_loss, R_train_masked, R_hat, N)\n",
    "        train_losses.append(np_train_loss)\n",
    "        \n",
    "        # Eval\n",
    "        val_loss, np_val_loss, _, _ = MF_model.calculate_loss(R_val.copy(), R_hat)\n",
    "        val_losses.append(np_val_loss)\n",
    "        print(f'Epoch {curr_epoch+1} losses -> Train: {np_train_loss} - Val: {np_val_loss}')\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationPytorchNonLinearModel(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, K, lr):\n",
    "        super().__init__()\n",
    "        self.is_linear = False\n",
    "        \n",
    "        self.U_layer = torch.nn.Linear(num_users, K)\n",
    "        torch.nn.init.xavier_uniform_(self.U_layer.weight)\n",
    "        self.V_layer = torch.nn.Linear(num_items, K)\n",
    "        torch.nn.init.xavier_uniform_(self.V_layer.weight)\n",
    "        \n",
    "        self.MSE = torch.nn.MSELoss()\n",
    "    \n",
    "    def forward(self, U_one_hots, V_one_hots):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def calculate_loss(self, R, R_hat):\n",
    "        t_loss = self.MSE(R, R_hat)\n",
    "        np_loss = convert_tensor_to_np(t_loss)\n",
    "        return t_loss, np_loss\n",
    "    \n",
    "    def optimize(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNonLinearModel(MatrixFactorizationPytorchNonLinearModel):\n",
    "    def __init__(self, num_users, num_items, K, lr):\n",
    "        super().__init__(num_users, num_items, K, lr)        \n",
    "        self.fc1 = torch.nn.Linear(2 * K, K)\n",
    "        self.fc2 = torch.nn.Linear(K, 1)\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, U_one_hots, V_one_hots):\n",
    "        U_latent = self.U_layer(U_one_hots)\n",
    "        V_latent = self.V_layer(V_one_hots)\n",
    "        x = torch.cat((U_latent, V_latent), dim=1)\n",
    "        # Non-linear transform\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        R_hat = self.fc2(x)\n",
    "        return R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNonLinearModel(MatrixFactorizationPytorchNonLinearModel):\n",
    "    def __init__(self, num_users, num_items, K, lr):\n",
    "        super().__init__(num_users, num_items, K, lr) \n",
    "        drop_p = 0.25\n",
    "        \n",
    "        self.U_dropout = torch.nn.Dropout(p=drop_p)\n",
    "        self.V_dropout = torch.nn.Dropout(p=drop_p)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(2 * K, K)\n",
    "        self.dropout1 = torch.nn.Dropout(p=drop_p)\n",
    "        self.fc2 = torch.nn.Linear(K, K // 2)\n",
    "        self.dropout2 = torch.nn.Dropout(p=drop_p)\n",
    "        self.fc3 = torch.nn.Linear(K // 2, 1)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, U_one_hots, V_one_hots):\n",
    "        U_latent = self.U_layer(U_one_hots)\n",
    "        U_latent = self.U_dropout(U_latent)\n",
    "        V_latent = self.V_layer(V_one_hots)\n",
    "        V_latent = self.V_dropout(V_latent)\n",
    "        x = torch.cat((U_latent, V_latent), dim=1)\n",
    "        # Non-linear transform\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        R_hat = self.fc3(x)\n",
    "        return R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_and_targets(R, num_users, num_items):\n",
    "    def create_one_hot(length, pos):\n",
    "        one_hot = np.zeros(shape=(length,), dtype=np.float32)\n",
    "        one_hot[pos] = 1.\n",
    "        return one_hot\n",
    "    # Inputs are one hot vectors\n",
    "    U_one_hots, V_one_hots = [], []\n",
    "    # Targets are flattened R without nans\n",
    "    R_targets = []\n",
    "    for row in range(num_users):\n",
    "        row_one_hot = create_one_hot(num_users, row)\n",
    "        for col in range(num_items):\n",
    "            value = R[row, col]\n",
    "            if not np.isnan(value):\n",
    "                U_one_hots.append(row_one_hot)\n",
    "                V_one_hots.append(create_one_hot(num_items, col))\n",
    "                R_targets.append(value)\n",
    "    return torch.tensor(U_one_hots),torch.tensor(V_one_hots), torch.tensor(R_targets, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nonlinear_model(MF_model, R_train, R_val, num_users, num_items, num_epochs):\n",
    "    # Dont use matrices here as there can be a lot of \"white space\" in a sparse ranking matrix\n",
    "    U_one_hots_train, V_one_hots_train, R_targets_train = create_inputs_and_targets(R_train, num_users, num_items)\n",
    "    U_one_hots_val, V_one_hots_val, R_targets_val = create_inputs_and_targets(R_val, num_users, num_items)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    # Training\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        # Train\n",
    "        MF_model.train()  # train mode\n",
    "        # Reconstruct R_hat from latent factor matrices\n",
    "        R_hat_train = MF_model.forward(U_one_hots_train, V_one_hots_train)\n",
    "        # Calc MSE loss of this reconstruction\n",
    "        train_loss, np_train_loss = MF_model.calculate_loss(R_targets_train, R_hat_train)\n",
    "        # Calc grad and update\n",
    "        MF_model.optimize(train_loss)\n",
    "        train_losses.append(np_train_loss)\n",
    "        \n",
    "        # Eval\n",
    "        MF_model.eval()  # eval mode\n",
    "        R_hat_val = MF_model.forward(U_one_hots_val, V_one_hots_val)\n",
    "        val_loss, np_val_loss = MF_model.calculate_loss(R_targets_val, R_hat_val)\n",
    "        val_losses.append(np_val_loss)\n",
    "        print(f'Epoch {curr_epoch+1} losses -> Train: {np_train_loss} - Val: {np_val_loss}')\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'scratch-linear': MatrixFactorizationCustomLinearModel, \n",
    "              'pytorch-linear': MatrixFactorizationPytorchLinearModel,\n",
    "              'pytorch-nonlinear-simple': SimpleNonLinearModel, \n",
    "              'pytorch-nonlinear-complex': ComplexNonLinearModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_definition, performance_dict):\n",
    "    model = model_dict[model_definition](num_users, num_items, K, lr)\n",
    "    print(f'Training {model_definition} model...')\n",
    "    if model.is_linear:\n",
    "        train_losses, val_losses = train_linear_model(model, R_train, R_val, num_epochs)\n",
    "    else:\n",
    "        train_losses, val_losses = \\\n",
    "            train_nonlinear_model(model, R_train, R_val, num_users, num_items, num_epochs)\n",
    "    print('...completed training\\n')\n",
    "    performance_dict[model_definition] = (train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models():\n",
    "    performance_dict = OrderedDict()\n",
    "    # Run all models, or only the specified models\n",
    "    model_list = list(model_dict.keys()) if run_all_models else model_definitions\n",
    "    for model_definition in model_list:\n",
    "        train_model(model_definition, performance_dict)\n",
    "    return performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and train model(s)\n",
    "performance_dict = train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(performance_dict, save_file='performance.png'):\n",
    "    fig, ax = plt.subplots()\n",
    "    for model_name, (train_losses, val_losses) in list(performance_dict.items()):\n",
    "        epochs = list(range(1, len(train_losses)+1))\n",
    "        ax.plot(epochs, val_losses, label=model_name, linewidth=3)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('val loss')\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0, 15)\n",
    "    \n",
    "    plt.xticks([1] + list(range(10, 100, 10))+[100])\n",
    "    \n",
    "    plt.yticks([0, 1, 3, 5, 7, 9, 11, 13, 15])\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title('Val Loss by Epoch of MF Models')\n",
    "    fig.savefig(save_file)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(performance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
